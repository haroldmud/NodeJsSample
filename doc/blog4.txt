Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.

Protocols allow the client and server to speak to each other in a common language by providing a set of communication rules. There are numerous kinds of protocols, but some of the most familiar are FTP (File Transfer Protocol) and HTTP (HyperText Transfer Protocol). There is also TCP (Transmission Control Protocol) which sends data via packets.

What exactly are packets? Packets are small amounts of data being transferred over a network with the structure being determined by the aforementioned protocols. Packets commonly include a header and payload with headers comprising of information such as the source, destination, and other details about the packet itself. The payload contains the actual data being transferred.

I used to hear packets often in terms of packet loss, which is when some amount of the sent packets fail to meet their destination. Issues with packets can cause latency and jitters — resulting in unnatural delays in audio or video.

Streams and Buffers
I’m sure everyone has experienced their streaming services buffering when they have poor internet connection. This relates back to the form of streams and buffering I’m discussing in this post. A stream is the data flow between two points and the buffers are the small collections of data being transferred between them.

As opposed to downloading a full file, waiting for the download to complete, and then having access to the entirety of the media; streaming allows you to access the data as it is actively transferred to your client endpoint. Since streaming sends small bits of data at a time, it does not require the collecting and sending all of the information before use.

Instead, the buffers act as a temporary storage spot for smaller chunks of data. Once the buffer is full, it is passed along the stream. Buffering in a negative connotation comes from slow connections that have to wait to receive these chunks of data before it can process and display the information. Your computer receives a buffer full of data, this section is processed and played, and then it has to wait again for an additional buffer of information.

Connecting this all to Node.js
Node.js allows a user to create streams and introduced the buffer class for utilization with server-side code. The Buffer class is similar to an array and deals with binary data directly. So buffers are the method you can send and receive data via streams. Node.js has a few different types of streams — writable, readable, duplex, and transform.

Writable streams allow Node to write to a stream, readable allows data to a be read from a stream, and duplex are streams that can are both readable and writable. Transform streams are a bit more complicated as they are duplex streams that allow the data to be modified as it is written or read.

One of the core modules (fs) in Node.js allows you to work with the file system on your computer. By using native methods provided by requiring fs, you can create read and write streams with ease.

let fs = require('fs')
let read = fs.createReadStream(__dirname + '/textFile.txt', 'utf8');
let writeStr = fs.createWriteStream(__dirname + '/writeFile.txt');
read.on('data', function(chunk){
   console.log('new chunk received');
   writeStr.write(chunk);
});
In the above example, we are creating a read stream by specifying which file we want to read and the encoding. We are also creating a write stream with the destination file path. The .createReadStream() method inherits from the event emitter, so we create an event listener for when a chunk of data is received. On the reception of a new data buffer, we console.log a confirmation string and enact the .write() method on our created writeStr stream to our predetermined file path. The content from the text file we read, is then written to our new text file from the write stream.

Node also has a pipe method which directly takes data from a readable stream and sends it to a writable stream. Using a pipe, you do not have to manually listen for a data event and write to a stream.

An example of this is below:

let http = require('http');
let fs = require('fs');
let server = http.createServer(function(request, response){
  response.writeHead(200, {'Content-Type': 'text/plain'});
  var read = fs.createReadStream(__dirname + 'readMe.txt', 'utf8');
  read.pipe(response);
});


Instead of manually creating the .createReadStream() and .createWriteStream() methods with the data event, we can incorporate the pipe method into server creation. When we spin up the server, the text content from the readMe.txt file is piped into the response stream and displayed on our local host page.

Hopefully this post has provided a good understanding of the basics of network data flow and how transferring small chunks of data like buffers can increase performance. Thanks for reading!Recently I’ve started looking into Node.js since the bulk of my back-end experience is in Ruby on Rails. One of the concepts that came up was buffers and streams so I thought I would put together a post going a bit more in depth into these concepts. Not only is it helpful with learning Node — which allows users to utilize readable/writable streams — but it can also be relevant to anyone who has streamed data such as music or video.

Packets
Before diving right into buffers and streams, it’s important to have a basic understanding of how data is transferred between two points — namely the client and a server. Data is shared between the client and the server by a series of requests and responses and connected via a socket. A socket is basically a channel of communication opened between the client and server IP addresses that allow data to be shared between them.